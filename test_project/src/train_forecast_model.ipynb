{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1db329b-a610-438e-8516-a3c93d65dd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Install Dependencies\n",
    "!pip install prophet\n",
    "!pip install databricks-sdk --upgrade\n",
    "!pip install mlflow\n",
    "!pip install grpcio\n",
    "!pip install grpcio-status\n",
    "!pip install pandas\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c44b3-9424-4b6b-88ef-1b6f06f6e044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: update the catalog, schema, and table name for your data and give your model a name\n",
    "catalog = \"johannes_oehler\"\n",
    "schema = \"vectorlab\"\n",
    "table = \"forecast_data\"\n",
    "forecast_horizon = 10\n",
    "\n",
    "\n",
    "# Define the catalog, schema, and model name for organizing the model within the MLflow model registry\n",
    "model_catalog = \"johannes_oehler\" #Update it to your catalog name\n",
    "model_schema = \"vectorlab\" #Update it to your schema name\n",
    "model_name = \"prophet_forecast\" #Update it to your model name\n",
    "\n",
    "serving_endpoint_name = \"forecast_joe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ac0d1f-8dbf-408e-926e-e4fcab6e60bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select Data\n",
    "query = f\"SELECT date, store, SUM(sales) as sales FROM {catalog}.{schema}.{table} GROUP BY date, store ORDER BY date desc\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "# Choose a single store to make the calculations simpler\n",
    "df = df.filter(df.store == 1)\n",
    "\n",
    "# train-test-split\n",
    "train_df = df.orderBy(df.date.asc()).limit(df.count() - forecast_horizon).orderBy(df.date.desc())\n",
    "test_df = df.orderBy(df.date.desc()).limit(forecast_horizon).toPandas()\n",
    "\n",
    "train_df.show(5)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ac82ba-8cef-4174-bd58-a74ba27c30a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Dropping rows with missing values in the 'sales' column\n",
    "cleaned_df = train_df.na.drop(subset=[\"sales\"]) \n",
    "cleaned_df.show(5)\n",
    "\n",
    "# Calculating IQR and defining bounds for outliers\n",
    "quartiles = cleaned_df.approxQuantile(\"sales\", [0.25, 0.75], 0.05) \n",
    "IQR = quartiles[1] - quartiles[0]\n",
    "lower_bound = 0\n",
    "upper_bound = quartiles[1] + 1.5 * IQR\n",
    "\n",
    "# Filtering out outliers\n",
    "no_outliers_df = cleaned_df.filter(\n",
    "    (col(\"sales\") > lit(lower_bound)) \n",
    "    & (col(\"sales\") <= lit(upper_bound)) \n",
    ")\n",
    "\n",
    "# Showing the updated DataFrame\n",
    "no_outliers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1c8a25-b155-4200-8023-734ca77752e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Prophet requires at the minimum 2 columns - ds & y\n",
    "train_df = no_outliers_df.select(to_date(col(\"date\")).alias(\"ds\"), col(\"store\"), col(\"sales\").alias(\"y\").cast(\"double\")).orderBy(col(\"ds\").desc())\n",
    "\n",
    "# set model parameters\n",
    "prophet_model = Prophet(\n",
    "  interval_width=0.95,\n",
    "  growth='linear',\n",
    "  daily_seasonality=True,\n",
    "  weekly_seasonality=True,\n",
    "  yearly_seasonality=True,\n",
    "  seasonality_mode='additive'\n",
    "  )\n",
    " \n",
    "# fit the model to historical data\n",
    "history_pd = train_df.toPandas()\n",
    "prophet_model.fit(history_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5164bfa-a3b0-4bed-8bbf-d219454cf2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, to_date\n",
    "from prophet import Prophet\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel, log_model\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba1d9a8-faec-431d-bb80-32f9934533a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProphetWrapper(PythonModel):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        future_pd = self.model.make_future_dataframe(\n",
    "            periods=forecast_horizon,\n",
    "            freq=\"d\",\n",
    "            include_history=True\n",
    "        )\n",
    "        forecast_pd = self.model.predict(future_pd)\n",
    "        return forecast_pd[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "\n",
    "wrapped_model = ProphetWrapper(prophet_model)\n",
    "\n",
    "with mlflow.start_run(run_name=\"prophet_training\") as run:\n",
    "    # Input/output examples for signature\n",
    "    input_example = history_pd.head()[[\"ds\", \"y\"]]\n",
    "    output_example = prophet_model.predict(input_example).iloc[:10]\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "\n",
    "    # Log the model artifact\n",
    "    model_path = \"forecasting_model\"\n",
    "    log_model(\n",
    "        artifact_path=model_path,\n",
    "        python_model=wrapped_model,\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    # Log hyperparameters / metadata\n",
    "    mlflow.log_param(\"interval_width\", prophet_model.interval_width)\n",
    "    mlflow.log_param(\"growth\", prophet_model.growth)\n",
    "    mlflow.log_param(\"seasonality_mode\", prophet_model.seasonality_mode)\n",
    "    mlflow.log_param(\"daily_seasonality\", prophet_model.daily_seasonality)\n",
    "    mlflow.log_param(\"weekly_seasonality\", prophet_model.weekly_seasonality)\n",
    "    mlflow.log_param(\"yearly_seasonality\", prophet_model.yearly_seasonality)\n",
    "\n",
    "    # Save run_id for evaluation\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Training run logged with run_id: {run_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e37c25-20f7-4708-8121-367e7972d1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel, log_model\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Save the fitted Prophet model as a pickle file\n",
    "with open(\"/tmp/prophet_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(prophet_model, f)\n",
    "\n",
    "# Wrapper loads the pickled model\n",
    "class ProphetWrapper(PythonModel):\n",
    "    def __init__(self, model_path, forecast_horizon):\n",
    "        self.model_path = model_path\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "    def load_context(self, context):\n",
    "        import pickle\n",
    "        # Load the fitted Prophet model\n",
    "        with open(self.model_path, \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Generate future dataframe\n",
    "        future_pd = self.model.make_future_dataframe(\n",
    "            periods=self.forecast_horizon,\n",
    "            freq=\"d\",\n",
    "            include_history=True\n",
    "        )\n",
    "        forecast_pd = self.model.predict(future_pd)\n",
    "        return forecast_pd[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "\n",
    "# Wrap the saved model\n",
    "forecast_horizon = 10\n",
    "wrapped_model = ProphetWrapper(\"/tmp/prophet_model.pkl\", forecast_horizon)\n",
    "\n",
    "# Start MLflow run and log the model\n",
    "with mlflow.start_run(run_name=\"prophet_training\") as run:\n",
    "    input_example = history_pd.head()[[\"ds\", \"y\"]]\n",
    "    output_example = prophet_model.predict(input_example).iloc[:10]\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "\n",
    "    log_model(\n",
    "        artifact_path=\"forecasting_model\",\n",
    "        python_model=wrapped_model,\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Training run logged with run_id: {run_id}\")\n",
    "\n",
    "# Verify artifacts\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "artifacts = client.list_artifacts(run_id)\n",
    "for a in artifacts:\n",
    "    print(a.path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train_forecast_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
