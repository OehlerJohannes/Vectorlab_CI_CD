{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Model Validation Notebook\n",
    "##\n",
    "# This notebook uses mlflow.evaluate API to run model validation after training\n",
    "# a model in model registry, before deploying it to the \"champion\" alias.\n",
    "#\n",
    "# Parameters:\n",
    "#\n",
    "# * env                         - Environment (dev, staging, prod)\n",
    "# * run_mode                    - disabled/dry_run/enabled\n",
    "# * enable_baseline_comparison  - Compare against champion model\n",
    "# * catalog/schema/table        - Validation data location\n",
    "# * forecast_horizon            - Forecast horizon\n",
    "# * model_name                  - Three-level UC model name\n",
    "# * model_version               - Model version to validate\n",
    "# * experiment_name             - MLflow experiment\n",
    "##################################################################################\n",
    "\n",
    "# MAGIC %load_ext autoreload\n",
    "# MAGIC %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Install dependencies\n",
    "# MAGIC %pip install prophet databricks-sdk mlflow pandas\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Notebook arguments\n",
    "dbutils.widgets.text(\"experiment_name\", \"/dev-prophet-forecast-experiment\", \"Experiment Name\")\n",
    "dbutils.widgets.dropdown(\"run_mode\", \"dry_run\", [\"disabled\", \"dry_run\", \"enabled\"], \"Run Mode\")\n",
    "dbutils.widgets.dropdown(\"enable_baseline_comparison\", \"false\", [\"true\", \"false\"], \"Enable Baseline Comparison\")\n",
    "dbutils.widgets.text(\"catalog\", \"johannes_oehler\", label=\"Data Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"vectorlab\", label=\"Data Schema\")\n",
    "dbutils.widgets.text(\"table\", \"forecast_data\", label=\"Data Table\")\n",
    "dbutils.widgets.text(\"forecast_horizon\", \"10\", label=\"Forecast Horizon\")\n",
    "dbutils.widgets.text(\"model_name\", \"johannes_oehler.vectorlab.prophet_forecast\", \"Model Name\")\n",
    "dbutils.widgets.text(\"model_version\", \"\", \"Model Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Check run mode\n",
    "run_mode = dbutils.widgets.get(\"run_mode\").lower()\n",
    "assert run_mode == \"disabled\" or run_mode == \"dry_run\" or run_mode == \"enabled\"\n",
    "\n",
    "if run_mode == \"disabled\":\n",
    "    print(\"Model validation is in DISABLED mode. Exit model validation without blocking model deployment.\")\n",
    "    dbutils.notebook.exit(0)\n",
    "    \n",
    "dry_run = run_mode == \"dry_run\"\n",
    "\n",
    "if dry_run:\n",
    "    print(\"Model validation is in DRY_RUN mode. Validation threshold failures will not block deployment.\")\n",
    "else:\n",
    "    print(\"Model validation is in ENABLED mode. Validation threshold failures will block deployment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Setup MLflow and get model info\n",
    "import mlflow\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get model info from training task or widgets\n",
    "model_uri = dbutils.jobs.taskValues.get(\"Train\", \"model_uri\", debugValue=\"\")\n",
    "model_name = dbutils.jobs.taskValues.get(\"Train\", \"model_name\", debugValue=\"\")\n",
    "model_version = dbutils.jobs.taskValues.get(\"Train\", \"model_version\", debugValue=\"\")\n",
    "\n",
    "# Fall back to widgets if not running in a workflow\n",
    "if model_uri == \"\":\n",
    "    model_name = dbutils.widgets.get(\"model_name\")\n",
    "    model_version = dbutils.widgets.get(\"model_version\")\n",
    "    model_uri = \"models:/\" + model_name + \"/\" + model_version\n",
    "\n",
    "baseline_model_uri = \"models:/\" + model_name + \"@champion\"\n",
    "\n",
    "assert model_uri != \"\", \"model_uri notebook parameter must be specified\"\n",
    "assert model_name != \"\", \"model_name notebook parameter must be specified\"\n",
    "assert model_version != \"\", \"model_version notebook parameter must be specified\"\n",
    "\n",
    "print(f\"Validating model: {model_uri}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Model version: {model_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Get validation parameters\n",
    "enable_baseline_comparison = dbutils.widgets.get(\"enable_baseline_comparison\")\n",
    "assert enable_baseline_comparison == \"true\" or enable_baseline_comparison == \"false\"\n",
    "enable_baseline_comparison = enable_baseline_comparison == \"true\"\n",
    "\n",
    "# Get data parameters\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "forecast_horizon = int(dbutils.widgets.get(\"forecast_horizon\"))\n",
    "\n",
    "print(f\"Validation data source: {catalog}.{schema}.{table}\")\n",
    "print(f\"Forecast horizon: {forecast_horizon}\")\n",
    "print(f\"Baseline comparison enabled: {enable_baseline_comparison}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Load and prepare validation data\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit, to_date\n",
    "\n",
    "# Load data\n",
    "query = f\"SELECT date, store, SUM(sales) as sales FROM {catalog}.{schema}.{table} GROUP BY date, store ORDER BY date desc\"\n",
    "df = spark.sql(query)\n",
    "\n",
    "# Filter to single store for simplicity\n",
    "df = df.filter(df.store == 1)\n",
    "\n",
    "# Get test data (most recent forecast_horizon days)\n",
    "test_df = df.orderBy(df.date.desc()).limit(forecast_horizon)\n",
    "\n",
    "# Clean data - remove missing values\n",
    "cleaned_df = test_df.na.drop(subset=[\"sales\"]) \n",
    "\n",
    "# Remove outliers using IQR method\n",
    "quartiles = cleaned_df.approxQuantile(\"sales\", [0.25, 0.75], 0.05) \n",
    "IQR = quartiles[1] - quartiles[0]\n",
    "lower_bound = 0\n",
    "upper_bound = quartiles[1] + 1.5 * IQR\n",
    "\n",
    "no_outliers_df = cleaned_df.filter(\n",
    "    (col(\"sales\") > lit(lower_bound)) \n",
    "    & (col(\"sales\") <= lit(upper_bound)) \n",
    ")\n",
    "\n",
    "# Prepare data in format expected by Prophet model (ds, y columns)\n",
    "validation_df = no_outliers_df.select(\n",
    "    to_date(col(\"date\")).alias(\"ds\"), \n",
    "    col(\"sales\").alias(\"y\").cast(\"double\")\n",
    ").orderBy(col(\"ds\").asc())\n",
    "\n",
    "# Convert to Pandas for mlflow.evaluate\n",
    "validation_data = validation_df.toPandas()\n",
    "validation_data[\"ds\"] = pd.to_datetime(validation_data[\"ds\"])\n",
    "\n",
    "print(f\"Validation dataset size: {len(validation_data)} records\")\n",
    "validation_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Define validation thresholds\n",
    "from mlflow.models import MetricThreshold\n",
    "\n",
    "# Define validation thresholds for forecasting\n",
    "# These will determine if the model passes validation\n",
    "validation_thresholds = {\n",
    "    \"mean_squared_error\": MetricThreshold(\n",
    "        threshold=1000,  # MSE should be <= 1000\n",
    "        greater_is_better=False,\n",
    "    ),\n",
    "    \"mean_absolute_error\": MetricThreshold(\n",
    "        threshold=25,  # MAE should be <= 25\n",
    "        greater_is_better=False,\n",
    "    ),\n",
    "    \"root_mean_squared_error\": MetricThreshold(\n",
    "        threshold=30,  # RMSE should be <= 30\n",
    "        greater_is_better=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define custom metrics (optional)\n",
    "custom_metrics = []\n",
    "\n",
    "# Evaluator config (optional)\n",
    "evaluator_config = {}\n",
    "\n",
    "print(\"Validation thresholds configured:\")\n",
    "for metric_name, threshold in validation_thresholds.items():\n",
    "    print(f\"  {metric_name}: <= {threshold.threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Helper functions\n",
    "def get_run_link(run_info):\n",
    "    return \"[Run](#mlflow/experiments/{0}/runs/{1})\".format(\n",
    "        run_info.experiment_id, run_info.run_id\n",
    "    )\n",
    "\n",
    "\n",
    "def get_training_run(model_name, model_version):\n",
    "    version = client.get_model_version(model_name, model_version)\n",
    "    return mlflow.get_run(run_id=version.run_id)\n",
    "\n",
    "\n",
    "def generate_run_name(training_run):\n",
    "    return None if not training_run else training_run.info.run_name + \"-validation\"\n",
    "\n",
    "\n",
    "def generate_description(training_run):\n",
    "    return (\n",
    "        None\n",
    "        if not training_run\n",
    "        else \"Model Training Details: {0}\\n\".format(get_run_link(training_run.info))\n",
    "    )\n",
    "\n",
    "\n",
    "def log_to_model_description(run, success):\n",
    "    run_link = get_run_link(run.info)\n",
    "    description = client.get_model_version(model_name, model_version).description\n",
    "    status = \"SUCCESS\" if success else \"FAILURE\"\n",
    "    if description != \"\" and description is not None:\n",
    "        description += \"\\n\\n---\\n\\n\"\n",
    "    else:\n",
    "        description = \"\"\n",
    "    description += \"Model Validation Status: {0}\\nValidation Details: {1}\".format(\n",
    "        status, run_link\n",
    "    )\n",
    "    client.update_model_version(\n",
    "        name=model_name, version=model_version, description=description\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Run model validation with mlflow.evaluate\n",
    "import os\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "training_run = get_training_run(model_name, model_version)\n",
    "\n",
    "# Run mlflow.evaluate\n",
    "with mlflow.start_run(\n",
    "    run_name=generate_run_name(training_run),\n",
    "    description=generate_description(training_run),\n",
    ") as run, tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    \n",
    "    # Log validation thresholds\n",
    "    validation_thresholds_file = os.path.join(tmp_dir, \"validation_thresholds.txt\")\n",
    "    with open(validation_thresholds_file, \"w\") as f:\n",
    "        if validation_thresholds:\n",
    "            for metric_name in validation_thresholds:\n",
    "                f.write(\n",
    "                    \"{0:30}  {1}\\n\".format(\n",
    "                        metric_name, str(validation_thresholds[metric_name])\n",
    "                    )\n",
    "                )\n",
    "    mlflow.log_artifact(validation_thresholds_file)\n",
    "\n",
    "    try:\n",
    "        # Run mlflow.evaluate\n",
    "        eval_result = mlflow.evaluate(\n",
    "            model=model_uri,\n",
    "            data=validation_data,\n",
    "            targets=\"y\",\n",
    "            model_type=\"regressor\",\n",
    "            evaluators=\"default\",\n",
    "            validation_thresholds=validation_thresholds,\n",
    "            custom_metrics=custom_metrics,\n",
    "            baseline_model=None\n",
    "            if not enable_baseline_comparison\n",
    "            else baseline_model_uri,\n",
    "            evaluator_config=evaluator_config,\n",
    "        )\n",
    "        \n",
    "        # Log metrics comparison\n",
    "        metrics_file = os.path.join(tmp_dir, \"metrics.txt\")\n",
    "        with open(metrics_file, \"w\") as f:\n",
    "            f.write(\n",
    "                \"{0:30}  {1:30}  {2}\\n\".format(\"metric_name\", \"candidate\", \"baseline\")\n",
    "            )\n",
    "            for metric in eval_result.metrics:\n",
    "                candidate_metric_value = str(eval_result.metrics[metric])\n",
    "                baseline_metric_value = \"N/A\"\n",
    "                if metric in eval_result.baseline_model_metrics:\n",
    "                    mlflow.log_metric(\n",
    "                        \"baseline_\" + metric, eval_result.baseline_model_metrics[metric]\n",
    "                    )\n",
    "                    baseline_metric_value = str(\n",
    "                        eval_result.baseline_model_metrics[metric]\n",
    "                    )\n",
    "                f.write(\n",
    "                    \"{0:30}  {1:30}  {2}\\n\".format(\n",
    "                        metric, candidate_metric_value, baseline_metric_value\n",
    "                    )\n",
    "                )\n",
    "        mlflow.log_artifact(metrics_file)\n",
    "        log_to_model_description(run, True)\n",
    "        \n",
    "        # Assign \"challenger\" alias to indicate model version has passed validation checks\n",
    "        print(\"Validation checks passed. Assigning 'challenger' alias to model version.\")\n",
    "        client.set_registered_model_alias(model_name, \"challenger\", model_version)\n",
    "        \n",
    "        print(\"\\n=== Validation Results ===\")\n",
    "        print(f\"Model: {model_uri}\")\n",
    "        print(f\"Status: PASSED\")\n",
    "        print(f\"\\nKey Metrics:\")\n",
    "        for metric_name in [\"mean_squared_error\", \"mean_absolute_error\", \"root_mean_squared_error\"]:\n",
    "            if metric_name in eval_result.metrics:\n",
    "                print(f\"  {metric_name}: {eval_result.metrics[metric_name]:.4f}\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        log_to_model_description(run, False)\n",
    "        error_file = os.path.join(tmp_dir, \"error.txt\")\n",
    "        with open(error_file, \"w\") as f:\n",
    "            f.write(\"Validation failed : \" + str(err) + \"\\n\")\n",
    "            f.write(traceback.format_exc())\n",
    "        mlflow.log_artifact(error_file)\n",
    "        \n",
    "        print(\"\\n=== Validation Results ===\")\n",
    "        print(f\"Model: {model_uri}\")\n",
    "        print(f\"Status: FAILED\")\n",
    "        print(f\"Error: {str(err)}\")\n",
    "        \n",
    "        if not dry_run:\n",
    "            raise err\n",
    "        else:\n",
    "            print(\n",
    "                \"\\nModel validation failed in DRY_RUN mode. It will not block model deployment.\"\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
