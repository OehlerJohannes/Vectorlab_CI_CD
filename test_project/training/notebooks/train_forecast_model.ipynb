{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1db329b-a610-438e-8516-a3c93d65dd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Prophet Forecasting Model Training Notebook\n",
    "#\n",
    "# This notebook trains a Prophet forecasting model and registers it to Unity Catalog.\n",
    "#\n",
    "# Parameters:\n",
    "# * env (required)              - Environment the notebook is run in (dev, staging, or prod)\n",
    "# * catalog (required)          - Catalog name for the training data\n",
    "# * schema (required)           - Schema name for the training data  \n",
    "# * table (required)            - Table name for the training data\n",
    "# * forecast_horizon (required) - Number of periods to forecast\n",
    "# * experiment_name (required)  - MLflow experiment name for the training runs\n",
    "# * model_name (required)       - Three-level name (<catalog>.<schema>.<model_name>) to register the trained model in Unity Catalog\n",
    "# * serving_endpoint_name       - Optional name for the serving endpoint\n",
    "##################################################################################\n",
    "\n",
    "# MAGIC %load_ext autoreload\n",
    "# MAGIC %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Install dependencies\n",
    "# MAGIC %pip install prophet databricks-sdk mlflow grpcio grpcio-status pandas\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c44b3-9424-4b6b-88ef-1b6f06f6e044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1, Notebook arguments\n",
    "# List of input args needed to run this notebook as a job.\n",
    "# Provide them via DB widgets or notebook arguments.\n",
    "\n",
    "# Notebook Environment\n",
    "dbutils.widgets.dropdown(\"env\", \"dev\", [\"dev\", \"staging\", \"prod\"], \"Environment Name\")\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "# Training data location\n",
    "dbutils.widgets.text(\"catalog\", \"johannes_oehler\", label=\"Data Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"vectorlab\", label=\"Data Schema\")\n",
    "dbutils.widgets.text(\"table\", \"forecast_data\", label=\"Data Table\")\n",
    "\n",
    "# Forecast configuration\n",
    "dbutils.widgets.text(\"forecast_horizon\", \"10\", label=\"Forecast Horizon\")\n",
    "\n",
    "# MLflow experiment name\n",
    "dbutils.widgets.text(\n",
    "    \"experiment_name\",\n",
    "    f\"/dev-prophet-forecast-experiment\",\n",
    "    label=\"MLflow experiment name\",\n",
    ")\n",
    "\n",
    "# Unity Catalog registered model name (three-level namespace)\n",
    "dbutils.widgets.text(\n",
    "    \"model_name\", \n",
    "    \"johannes_oehler.vectorlab.prophet_forecast\", \n",
    "    label=\"Full (Three-Level) Model Name\"\n",
    ")\n",
    "\n",
    "# Optional: Serving endpoint name\n",
    "dbutils.widgets.text(\n",
    "    \"serving_endpoint_name\",\n",
    "    \"forecast_joe\",\n",
    "    label=\"Serving Endpoint Name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Define input and output variables\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "forecast_horizon = int(dbutils.widgets.get(\"forecast_horizon\"))\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "serving_endpoint_name = dbutils.widgets.get(\"serving_endpoint_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Set MLflow experiment and Unity Catalog registry\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Model will be registered to: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Helper function\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Get the latest version number for a registered model\"\"\"\n",
    "    latest_version = 1\n",
    "    mlflow_client = MlflowClient()\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ac0d1f-8dbf-408e-926e-e4fcab6e60bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1, Load and split data\n",
    "query = f\"SELECT date, store, SUM(sales) as sales FROM {catalog}.{schema}.{table} GROUP BY date, store ORDER BY date desc\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "# Choose a single store to make the calculations simpler\n",
    "df = df.filter(df.store == 1)\n",
    "\n",
    "# train-test-split\n",
    "train_df = df.orderBy(df.date.asc()).limit(df.count() - forecast_horizon).orderBy(df.date.desc())\n",
    "test_df = df.orderBy(df.date.desc()).limit(forecast_horizon).toPandas()\n",
    "\n",
    "train_df.show(5)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ac82ba-8cef-4174-bd58-a74ba27c30a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1, Clean data and remove outliers\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Dropping rows with missing values in the 'sales' column\n",
    "cleaned_df = train_df.na.drop(subset=[\"sales\"]) \n",
    "cleaned_df.show(5)\n",
    "\n",
    "# Calculating IQR and defining bounds for outliers\n",
    "quartiles = cleaned_df.approxQuantile(\"sales\", [0.25, 0.75], 0.05) \n",
    "IQR = quartiles[1] - quartiles[0]\n",
    "lower_bound = 0\n",
    "upper_bound = quartiles[1] + 1.5 * IQR\n",
    "\n",
    "# Filtering out outliers\n",
    "no_outliers_df = cleaned_df.filter(\n",
    "    (col(\"sales\") > lit(lower_bound)) \n",
    "    & (col(\"sales\") <= lit(upper_bound)) \n",
    ")\n",
    "\n",
    "# Showing the updated DataFrame\n",
    "no_outliers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1c8a25-b155-4200-8023-734ca77752e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1, Train Prophet model\n",
    "from prophet import Prophet\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Prophet requires at the minimum 2 columns - ds & y\n",
    "train_df = no_outliers_df.select(to_date(col(\"date\")).alias(\"ds\"), col(\"store\"), col(\"sales\").alias(\"y\").cast(\"double\")).orderBy(col(\"ds\").desc())\n",
    "\n",
    "# set model parameters\n",
    "prophet_model = Prophet(\n",
    "  interval_width=0.95,\n",
    "  growth='linear',\n",
    "  daily_seasonality=True,\n",
    "  weekly_seasonality=True,\n",
    "  yearly_seasonality=True,\n",
    "  seasonality_mode='additive'\n",
    "  )\n",
    " \n",
    "# fit the model to historical data\n",
    "history_pd = train_df.toPandas()\n",
    "prophet_model.fit(history_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5164bfa-a3b0-4bed-8bbf-d219454cf2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC Train a Prophet forecasting model, then log and register it to Unity Catalog with MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba1d9a8-faec-431d-bb80-32f9934533a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1, Create model wrapper and signature\n",
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "# Wrapper class for Prophet model - NO PICKLE NEEDED!\n",
    "# MLflow handles serialization automatically via save_model/load_model\n",
    "class ProphetWrapper(PythonModel):\n",
    "    def __init__(self, model, forecast_horizon):\n",
    "        \"\"\"\n",
    "        Initialize with the trained Prophet model directly.\n",
    "        No need for pickle - MLflow handles serialization.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate forecasts using the Prophet model.\n",
    "        \n",
    "        Args:\n",
    "            model_input: DataFrame with 'ds' and 'y' columns (historical data)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with forecast columns: ds, yhat, yhat_lower, yhat_upper\n",
    "        \"\"\"\n",
    "        # Generate future dataframe based on forecast horizon\n",
    "        future_pd = self.model.make_future_dataframe(\n",
    "            periods=self.forecast_horizon,\n",
    "            freq=\"d\",\n",
    "            include_history=True\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        forecast_pd = self.model.predict(future_pd)\n",
    "        \n",
    "        # Return relevant forecast columns\n",
    "        return forecast_pd[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "\n",
    "# Wrap the trained Prophet model (no pickle needed!)\n",
    "wrapped_model = ProphetWrapper(prophet_model, forecast_horizon)\n",
    "\n",
    "# Create input example and signature for MLflow\n",
    "input_example = history_pd.head()[[\"ds\", \"y\"]]\n",
    "signature = infer_signature(input_example, prophet_model.predict(input_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e37c25-20f7-4708-8121-367e7972d1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 1, Train and register model\n",
    "# Start MLflow run and log the model with Unity Catalog registration\n",
    "with mlflow.start_run(run_name=\"prophet_training\") as run:\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"forecast_horizon\", forecast_horizon)\n",
    "    mlflow.log_param(\"interval_width\", 0.95)\n",
    "    mlflow.log_param(\"growth\", \"linear\")\n",
    "    mlflow.log_param(\"daily_seasonality\", True)\n",
    "    mlflow.log_param(\"weekly_seasonality\", True)\n",
    "    mlflow.log_param(\"yearly_seasonality\", True)\n",
    "    mlflow.log_param(\"seasonality_mode\", \"additive\")\n",
    "    \n",
    "    # Log training data info\n",
    "    mlflow.log_param(\"training_data_source\", f\"{catalog}.{schema}.{table}\")\n",
    "    mlflow.log_param(\"training_samples\", len(history_pd))\n",
    "    \n",
    "    # Log the model and register it to Unity Catalog\n",
    "    # Specify explicit dependencies to ensure serving environment compatibility\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"prophet_model\",\n",
    "        python_model=wrapped_model,\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=model_name,  # This registers the model to Unity Catalog\n",
    "        pip_requirements=[\n",
    "            \"prophet\",\n",
    "            \"pandas<2.0.0\",  # Pin to pandas 1.x for compatibility\n",
    "            \"numpy\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Training run logged with run_id: {run_id}\")\n",
    "    print(f\"Model registered to Unity Catalog as: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1, Return model info for downstream tasks\n",
    "# Get the latest model version that was just registered\n",
    "model_version = get_latest_model_version(model_name)\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "# Set task values for downstream tasks (e.g., validation, deployment)\n",
    "dbutils.jobs.taskValues.set(\"model_uri\", model_uri)\n",
    "dbutils.jobs.taskValues.set(\"model_name\", model_name)\n",
    "dbutils.jobs.taskValues.set(\"model_version\", model_version)\n",
    "\n",
    "print(f\"Model URI: {model_uri}\")\n",
    "print(f\"Model Version: {model_version}\")\n",
    "\n",
    "# Exit with model URI for use in workflows\n",
    "dbutils.notebook.exit(model_uri)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train_forecast_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
